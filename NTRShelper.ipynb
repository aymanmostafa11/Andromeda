{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymanmostafa11/Andromeda/blob/main/NTRShelper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import glob\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import utils\n",
        "import requests\n",
        "import json\n",
        "import pickle\n",
        "import gensim.corpora.dictionary\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "import re\n",
        "import string\n",
        "import glob\n",
        "\n",
        "from gensim import utils\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "class Preprocssor():\n",
        "\n",
        "  # improved list from Stone, Denis, Kwantes (2010)\n",
        "  STOPWORDS = \"\"\"\n",
        "  a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be\n",
        "  became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can\n",
        "  cannot cant co computer con could couldnt cry de describe\n",
        "  detail did didn do does doesn doing don done down due during\n",
        "  each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen\n",
        "  fify fill find fire first five for former formerly forty found four from front full further get give go\n",
        "  had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie\n",
        "  if in inc indeed interest into is it its itself keep last latter latterly least less ltd\n",
        "  just\n",
        "  kg km\n",
        "  made make many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely\n",
        "  neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off\n",
        "  often on once one only onto or other others otherwise our ours ourselves out over own part per\n",
        "  perhaps please put rather re\n",
        "  quite\n",
        "  rather really regarding\n",
        "  same say see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten\n",
        "  than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under\n",
        "  until up unless upon us used using\n",
        "  various very very via\n",
        "  was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you\n",
        "  your yours yourself yourselves\n",
        "\n",
        "  second minute time figure equation test inches centimetres celsius fahrenheit function kilograms pounds lbs section iii part object tests\n",
        "  nasa sec fig min max maximum minimum use data unit measure symbol input material materials \n",
        "  \"\"\"\n",
        "  STOPWORDS = frozenset(w for w in STOPWORDS.split() if w)\n",
        "\n",
        "  def remove_stopwords(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return \" \".join(w for w in s.split() if w not in STOPWORDS)\n",
        "  RE_PUNCT = re.compile('([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
        "  def strip_punctuation(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return RE_PUNCT.sub(\" \", s)\n",
        "\n",
        "  strip_punctuation2 = strip_punctuation\n",
        "\n",
        "  RE_TAGS = re.compile(r\"<([^>]+)>\", re.UNICODE)\n",
        "  def strip_tags(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return RE_TAGS.sub(\"\",s)\n",
        "\n",
        "\n",
        "\n",
        "  def strip_short(s, minsize=3):\n",
        "      s = utils.to_unicode(s)\n",
        "      return \" \".join(e for e in s.split() if len(e) >= minsize)\n",
        "\n",
        "\n",
        "\n",
        "  RE_NUMERIC = re.compile(r\"[0-9]+\", re.UNICODE)\n",
        "  def strip_numeric(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return RE_NUMERIC.sub(\"\", s)\n",
        "\n",
        "\n",
        "\n",
        "  RE_NONALPHA = re.compile(r\"\\W\", re.UNICODE)\n",
        "  def strip_non_alphanum(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return RE_NONALPHA.sub(\" \", s)\n",
        "\n",
        "\n",
        "\n",
        "  RE_WHITESPACE = re.compile(r\"(\\s)+\", re.UNICODE)\n",
        "  def strip_multiple_whitespaces(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      return RE_WHITESPACE.sub(\" \", s)\n",
        "\n",
        "  RE_AL_NUM = re.compile(r\"([a-z]+)([0-9]+)\", flags=re.UNICODE)\n",
        "  RE_NUM_AL = re.compile(r\"([0-9]+)([a-z]+)\", flags=re.UNICODE)\n",
        "  def split_alphanum(s):\n",
        "      s = utils.to_unicode(s)\n",
        "      s = RE_AL_NUM.sub(r\"\\1 \\2\", s)\n",
        "      return RE_NUM_AL.sub(r\"\\1 \\2\", s)\n",
        "\n",
        "\n",
        "\n",
        "  def stem_text(text):\n",
        "      \"\"\"\n",
        "      Return lowercase and (porter-)stemmed version of string `text`.\n",
        "      \"\"\"\n",
        "      text = utils.to_unicode(text)\n",
        "      p = PorterStemmer()\n",
        "      return ' '.join(p.stem(word) for word in text.split())\n",
        "\n",
        "  stem = stem_text\n",
        "\n",
        "  DEFAULT_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces,\n",
        "                    strip_numeric, remove_stopwords, strip_short]\n",
        "\n",
        "\n",
        "  def preprocess_string(s, filters=DEFAULT_FILTERS):\n",
        "      s = utils.to_unicode(s)\n",
        "      for f in filters:\n",
        "          s = f(s)\n",
        "      return s.split()\n",
        "\n",
        "  def testingPreprocessing(doc):\n",
        "    #doc = str(doc.content)\n",
        "    data = list()\n",
        "    data.append(Preprocssor.preprocess_string(doc.replace('\\\\n', ' ').replace('-',' ')))\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for doc in data:\n",
        "      doc = [lemmatizer.lemmatize(word) for word in doc]  \n",
        "    return data\n",
        "\n",
        "  def doc2Bow(dataSet):\n",
        "    dictionary = gensim.corpora.Dictionary(dataSet)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in dataSet]\n",
        "\n",
        "    return bow_corpus , dictionary  "
      ],
      "metadata": {
        "id": "EjdKNIe31sva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MulA23lR19aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pWEkBWQtGim"
      },
      "outputs": [],
      "source": [
        "class NTRShelper:\n",
        "  preprocessor = Preprocssor()\n",
        "  __ntrs_api_root = 'https://ntrs.nasa.gov/api'\n",
        "  __hf_API_TOKEN = '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'\n",
        "  __topics_model = None\n",
        "  __hf_API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-xsum\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __query(self, document):\n",
        "    headers = {\"Authorization\": f\"Bearer {self.__hf_API_TOKEN}\"}\n",
        "    response = requests.post(self.__hf_API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "  def summarize(self, document):\n",
        "    \n",
        "    output = self.__query({\"inputs\": str(document),})\n",
        "    return output\n",
        "\n",
        "  \n",
        "  \n",
        "  def extractTopics(self, documents):\n",
        "    pass \n",
        "    testData = Preprocssor.testingPreprocessing(documents)\n",
        "    test_bow_corpus , test_dictionary = Preprocssor.doc2Bow(testData)\n",
        "    model = pickle.load(finalized_keyWords_model.sav)\n",
        "    for index  in sorted(model[test_bow_corpus]):\n",
        "      print(model.print_topic(index[0][0]))\n",
        "  \n",
        "  def fetch_document(self, document_id):\n",
        "    import requests\n",
        "    #url = self.__ntrs_api_root + f'/api/citations/{document_id}/downloads/{document_id}.txt'\n",
        "    url = f'https://ntrs.nasa.gov/api/citations/{document_id}/downloads/{document_id}.txt'\n",
        "    resp = requests.get(url)\n",
        "    if resp.status_code != 200 :\n",
        "      raise Exception(\"Document with specified id doesn't exist or isn't available for public\")\n",
        "    \n",
        "    return str(resp.content)\n",
        "\n",
        "  def apply(self, document_id):\n",
        "    doc = self.fetch_document(document_id)\n",
        "    print(self.summarize(doc))\n",
        "    #print(self.extractTopics(doc))\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NTRShelper().apply('19690031409')"
      ],
      "metadata": {
        "id": "oUaX_xmgunY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "d700f4a4-6878-4006-b322-73f579c2d845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7f6b5559354f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNTRShelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'19690031409'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-bdf9051c7418>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, document_id)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#print(self.summarize(doc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-bdf9051c7418>\u001b[0m in \u001b[0;36mextractTopics\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mextractTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocssor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestingPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtest_bow_corpus\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocssor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2Bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalized_keyWords_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-608b0330878e>\u001b[0m in \u001b[0;36mtestingPreprocessing\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#doc = str(doc.content)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreprocssor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-608b0330878e>\u001b[0m in \u001b[0;36mpreprocess_string\u001b[0;34m(s, filters)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m           \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-608b0330878e>\u001b[0m in \u001b[0;36mstrip_tags\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstrip_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mRE_TAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RE_TAGS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "resp = requests.get('https://ntrs.nasa.gov/api/citations/19690031409/downloads/19690031409.txt')"
      ],
      "metadata": {
        "id": "xzCoXtMgw4is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp.status_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIyq-JtxxPIh",
        "outputId": "273d6463-492b-4b85-a8f9-8194772a2768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJ8WpG_7yCZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}